{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from matplotlib import rc\n",
    "\n",
    "fontsize = 12\n",
    "\n",
    "# Make default font serif\n",
    "rc('font', family=b'serif', size=fontsize)\n",
    "# Hide legend frame, change size, make only one scatter marker show up\n",
    "rc('legend', borderpad=0.,labelspacing=0.25, fontsize=(fontsize - 1.), frameon=False, numpoints=1)\n",
    "# Default line width (e.g. for plot()) should be thinner than 1 point\n",
    "rc('patch', linewidth=0.75)\n",
    "rc('lines', linewidth=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BW1484 Core I Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_ref = 1.00020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denovo ouput file processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "class DenovoProcessor(object):\n",
    "    \"\"\"DenovoProcessor class: processes an output file from a Denovo calculation.\n",
    "       It can read a file, print that information to csv, and do some processing\n",
    "       for highlights.\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Set information we'll use in processing a Denovo output file\"\"\"  \n",
    "        \n",
    "        # Output variables that are in the header / problem specification\n",
    "        self.keys = ['problem_name', 'num cells I', 'num cells J', 'num cells K', 'cells', '  mesh ',\n",
    "                     'num_blocks_i', 'num_blocks_j', 'num_z_blocks', 'blocks', 'num_groups', \n",
    "                     'num upscatter groups', 'num_sets', 'domains', 'Pn_order', 'azimuthals_octant', \n",
    "                     'polars_octant', 'eq_set', 'DoF', ' tolerance', 'L2_tolerance', 'k_tolerance',\n",
    "                     'up_tol', 'eigen_solver', 'mg_preconditioner', 'relax_count', 'num_v_cycles', \n",
    "                     'relax_weight', 'ml_Pn', 'ml_az', 'ml_oct']\n",
    "        # Information about timing and processor distribution\n",
    "        self.time_keys = ['procs-per-node-x', 'procs-per-node-y', 'walltime']\n",
    "        # Iteration information that is printed during execution\n",
    "        # We'll use this to map into which column in a list the data needs to be enetered into\n",
    "        self.iter_dict = {'Iterations =' : 4, 'Eigenvalue Iteration   ' : 0,\n",
    "                          'relative error in k-eff of ' : 2,\n",
    "                          'absolute error in k-eff of ' : 3}  \n",
    "        self.k = {'eigenvalue of' : 1}\n",
    "        self.iter_info = np.empty([1,5])\n",
    "\n",
    "                \n",
    "    def parse_file(self, out_file):\n",
    "        \"\"\"Read the values for the keys from out_file\"\"\"\n",
    "        \n",
    "        # save file neame for later reporting\n",
    "        self.out_file = out_file\n",
    "        # Initalize values to match keys to None\n",
    "        self.vals = {key: None for key in self.keys}\n",
    "        self.time_vals = {key: None for key in self.time_keys}\n",
    "    \n",
    "        # Set flags and keys to use for items with duplicate entries\n",
    "        ml_flag = False\n",
    "        ml_keys = ['Pn_order', 'azimuthals_octant', 'polars_octant']\n",
    "        ml_doubles = {key: None for key in ml_keys}\n",
    "        ml_cnt = 0\n",
    "        up_db = False\n",
    "        #doubles = {'tolerance' : None}\n",
    "    \n",
    "        # open the file\n",
    "        if os.path.isfile(self.out_file):\n",
    "            f = open(self.out_file, 'r')\n",
    "        else:\n",
    "            raise IOError('could not find file', self.out_file)\n",
    "   \n",
    "        # We will use an iteration count index to make sure we're putting iter info into\n",
    "        # the correct row\n",
    "        iter_count = 0\n",
    "        \n",
    "        #try: \n",
    "        # Process the file\n",
    "        for line in f: \n",
    "            # exit if we've exceeded wall time or had some system error\n",
    "            #if re.search('aprun:', line):\n",
    "            #    raise StopIteration\n",
    "            # check for the duplicate items (that are in sub databases)\n",
    "            # before processing everything else\n",
    "            if ml_flag == True: \n",
    "                if ml_cnt != 3:\n",
    "                    for ml_key in ml_doubles:\n",
    "                        if re.search(ml_key, line):\n",
    "                            if ml_doubles[ml_key] == None:\n",
    "                                data = line.split(ml_key)\n",
    "                                ml_doubles[ml_key] = data[1].strip() \n",
    "                                ml_cnt = ml_cnt + 1\n",
    "\n",
    "            # We know that the upscatter tolerance is printed after the regular tolerance\n",
    "            # Thus, if up_db is true this tolerance is the upscatter tolerance. Also, store\n",
    "            # what's currently in tolerance to reset at the end\n",
    "            if up_db == True:\n",
    "                if re.search('tolerance', line):\n",
    "                    print 'up_tol', self.vals['up_tol'], 'tol', self.vals[' tolerance']\n",
    "                    if self.vals['up_tol'] == None:\n",
    "                        data = line.split('tolerance')\n",
    "                        self.vals['up_tol'] = data[1].strip()\n",
    "                        tol = self.vals[' tolerance']\n",
    "\n",
    "            # Information in the header\n",
    "            for key in self.vals: \n",
    "                # check for things to set the flags\n",
    "                if re.search('multilevel_quad_db', line):\n",
    "                    ml_flag = True\n",
    "                if re.search('upscatter_db', line):\n",
    "                    up_db = True   \n",
    "                # now look for the rest of the keys\n",
    "                if re.search(key, line):\n",
    "                    data = line.split(key)\n",
    "                    self.vals[key] = data[1].strip()  \n",
    "\n",
    "            # Time and CUDA information\n",
    "            for key in self.time_vals:\n",
    "                if re.search(key, line):\n",
    "                    data = line.split(key)\n",
    "                    num = data[1].strip().split()\n",
    "                    self.time_vals[key] = num[0].strip()         \n",
    "            # Iteration information\n",
    "            for key in self.iter_dict:\n",
    "                if re.search(key, line):\n",
    "                    data = line.split(key)\n",
    "                    num = data[1].strip().split()\n",
    "                    self.iter_info[iter_count][self.iter_dict[key]] = num[0].strip()\n",
    "                    # increment the counter after reading absolute error row, add an empty row\n",
    "                    if (key == 'absolute error in k-eff of '):\n",
    "                        iter_count = iter_count + 1\n",
    "                        self.iter_info = np.vstack((self.iter_info, np.empty([1,5])))\n",
    "            # k; need separately since it's on the same line as iteration #\n",
    "            for key in self.k:\n",
    "                if re.search(key, line):\n",
    "                    data = line.split(key)\n",
    "                    num = data[1].strip().split()\n",
    "                    self.iter_info[iter_count][self.k[key]] = num[0].strip()\n",
    "#        except StopIteration:\n",
    "#            pass\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        # calculate things we want in the spreadsheet\n",
    "        self.vals['cells']   = int(self.vals['num cells I']) * int(self.vals['num cells J']) * \\\n",
    "                               int(self.vals['num cells K'])\n",
    "        self.vals['blocks']  = int(self.vals['num_blocks_i']) * int(self.vals['num_blocks_j'])\n",
    "        self.vals['domains'] = int(self.vals['blocks']) * int(self.vals['num_sets'])\n",
    "        # set doubles\n",
    "        if ml_flag == True:\n",
    "            self.vals['ml_Pn']  = ml_doubles['Pn_order']\n",
    "            self.vals['ml_az']  = ml_doubles['azimuthals_octant']\n",
    "            self.vals['ml_oct'] = ml_doubles['polars_octant']\n",
    "        if up_db == True:\n",
    "            self.vals[' tolerance'] = tol\n",
    "        # the last iteration row gets filled with garbage; don't use it.\n",
    "        self.iter_info = self.iter_info[0:-1]\n",
    "        \n",
    "               \n",
    "    def print_csv(self, name):\n",
    "        \"\"\"Print the read in data to name.csv\"\"\"\n",
    "\n",
    "        # itereation keys we'd like to print since they have nicer names\n",
    "        iter_keys = ['eigen_iters', 'k', 'rel_err_k', 'abs_err_k', 'fxd_iters']\n",
    "        # store filename for printing later\n",
    "        self.csv = name \n",
    "\n",
    "        with open(self.csv+'.csv', 'wb') as csvfile:\n",
    "            # write the header / problem specification information\n",
    "            writer = csv.DictWriter(csvfile, fieldnames = self.keys, dialect='excel', \n",
    "                                    restval='',extrasaction='ignore')\n",
    "            #if not csv.Sniffer().has_header(csvfile.read(1024)):\n",
    "            writer.writeheader()\n",
    "            writer.writerow(self.vals)\n",
    "    \n",
    "            # Now write timing information\n",
    "            writer = csv.DictWriter(csvfile, fieldnames = self.time_keys, dialect='excel', \n",
    "                                    restval='',extrasaction='ignore')\n",
    "            writer.writeheader()\n",
    "            writer.writerow(self.time_vals)\n",
    "    \n",
    "            # And iteration information\n",
    "            # we need a regular writer for the list\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(iter_keys)\n",
    "            writer.writerows(self.iter_info)\n",
    "            print \"Wrote csv file\", self.csv, \"\\n\"\n",
    "            \n",
    "            \n",
    "# We want the k on the last iteration, the total number of fixed iters, last rel and abs err\n",
    "# eigen solver, preconditioner data, wall time\n",
    "    def highlights(self):\n",
    "        \"\"\"Print some highlights we care about most\"\"\"\n",
    "        \n",
    "        # Get total number of inner iterations\n",
    "        self.iter_dict['Iterations ='] = map(int, self.iter_dict['Iterations ='])\n",
    "        total_inner = np.sum(self.iter_dict['Iterations ='])\n",
    "        \n",
    "        # print\n",
    "        print \"From results file\", self.out_file, \"\\n\"\n",
    "        print \"After \",  self.time_vals['walltime'], \"minutes,\", \\\n",
    "        self.iter_dict['Eigenvalue Iteration   '][-1], \"eigenvalue iterations, and\", \\\n",
    "        total_inner, \"total inner iterations,\"\n",
    "        print \"k was\", self.iter_dict['eigenvalue of'][-1], \"with relative error\",\\\n",
    "        self.iter_dict['relative error in k-eff of '][-1], \"and absolute error\",\\\n",
    "        self.iter_dict['absolute error in k-eff of '][-1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a bunch of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_tol None tol 0.001\n",
      "Wrote csv file /home/slayer/Dropbox/Research/Calculations/Denovo/bw1484/bw1484_m20g8qr612_nosets \n",
      "\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "up_tol None tol 0.001\n",
      "Wrote csv file /home/slayer/Dropbox/Research/Calculations/Denovo/bw1484/bw1484_m20g8qr612 \n",
      "\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "up_tol None tol 0.001\n",
      "Wrote csv file /home/slayer/Dropbox/Research/Calculations/Denovo/bw1484/bw1484_m20g8qr612_pi \n",
      "\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "up_tol None tol 0.001\n",
      "Wrote csv file /home/slayer/Dropbox/Research/Calculations/Denovo/bw1484/bw1484_m20g8qr612_w1.1r3v2 \n",
      "\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "up_tol None tol 0.001\n",
      "Wrote csv file /home/slayer/Dropbox/Research/Calculations/Denovo/bw1484/bw1484_small \n",
      "\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Name of the file containing the output we want to process \n",
    "file_location = '/home/slayer/Dropbox/Research/Calculations/Denovo/bw1484'\n",
    "\n",
    "# Get a DenovoProcessor\n",
    "processor = DenovoProcessor()\n",
    "\n",
    "# Make a list of files to process\n",
    "files = ['bw1484_m20g8qr612_nosets.out', 'bw1484_m20g8qr612.out', 'bw1484_m20g8qr612_pi.out', \n",
    "         'bw1484_m20g8qr612_w1.1r3v2.out', 'bw1484_small.out']\n",
    "#file_name='bw1484_small.out' \n",
    "\n",
    "for file_name in files:\n",
    "    \n",
    "    full_out_file = os.path.join(file_location, file_name)\n",
    "\n",
    "    # parse the file, and write to csv\n",
    "    processor.__init__()\n",
    "    processor.parse_file(full_out_file)\n",
    "    processor.print_csv(full_out_file[0:-4])\n",
    "    #processor.highlights()\n",
    "    print \"\\n\\n-----------------------------\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
